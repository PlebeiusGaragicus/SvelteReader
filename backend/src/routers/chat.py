"""Chat router for handling AI chat requests.

This router handles:
- Sending messages to the LangGraph agent
- Streaming responses back to the frontend
- Thread management for conversation history
"""

import json
import os
from typing import AsyncGenerator

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from src.services.langgraph_client import LangGraphClient

router = APIRouter()

# Initialize the LangGraph client
langgraph_client = LangGraphClient(
    api_url=os.getenv("LANGGRAPH_API_URL", "http://localhost:2024"),
    assistant_id=os.getenv("LANGGRAPH_ASSISTANT_ID", "reader_assistant"),
)


class PassageContext(BaseModel):
    """Context about the passage being discussed."""

    text: str
    note: str | None = None
    book_title: str | None = None
    chapter: str | None = None


class ChatMessage(BaseModel):
    """A chat message from the user."""

    content: str
    thread_id: str | None = None
    passage_context: PassageContext | None = None


class ChatResponse(BaseModel):
    """Response from the chat endpoint."""

    content: str
    thread_id: str
    message_id: str


@router.post("/message")
async def send_message(message: ChatMessage) -> ChatResponse:
    """Send a message to the AI assistant and get a response.

    This is a non-streaming endpoint that waits for the full response.
    """
    try:
        # Create or get thread
        thread_id = message.thread_id
        if not thread_id:
            thread = await langgraph_client.create_thread()
            thread_id = thread["thread_id"]

        # Prepare the input for the agent
        input_data = {
            "messages": [{"role": "human", "content": message.content}],
        }

        if message.passage_context:
            input_data["passage_context"] = message.passage_context.model_dump()

        # Run the agent
        result = await langgraph_client.run(
            thread_id=thread_id,
            input_data=input_data,
        )

        # Extract the assistant's response
        messages = result.get("messages", [])
        if messages:
            last_message = messages[-1]
            return ChatResponse(
                content=last_message.get("content", ""),
                thread_id=thread_id,
                message_id=last_message.get("id", ""),
            )

        raise HTTPException(status_code=500, detail="No response from agent")

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/message/stream")
async def send_message_stream(message: ChatMessage):
    """Send a message and stream the response.

    This endpoint uses Server-Sent Events (SSE) to stream the response
    as it's generated by the LLM.
    """

    async def generate() -> AsyncGenerator[str, None]:
        try:
            # Create or get thread
            thread_id = message.thread_id
            if not thread_id:
                thread = await langgraph_client.create_thread()
                thread_id = thread["thread_id"]

            # Always send thread_id so client can track it
            yield f"data: {json.dumps({'type': 'thread_id', 'thread_id': thread_id})}\n\n"

            # Prepare the input for the agent
            input_data = {
                "messages": [{"role": "human", "content": message.content}],
            }

            if message.passage_context:
                input_data["passage_context"] = message.passage_context.model_dump()

            # Stream the response
            async for event in langgraph_client.stream(
                thread_id=thread_id,
                input_data=input_data,
            ):
                yield f"data: {json.dumps(event)}\n\n"

            # Send done event
            yield f"data: {json.dumps({'type': 'done'})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


@router.post("/thread")
async def create_thread():
    """Create a new conversation thread."""
    try:
        thread = await langgraph_client.create_thread()
        return {"thread_id": thread["thread_id"]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/thread/{thread_id}")
async def get_thread(thread_id: str):
    """Get the current state of a thread."""
    try:
        state = await langgraph_client.get_thread_state(thread_id)
        return state
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/thread/{thread_id}/messages")
async def get_thread_messages(thread_id: str):
    """Get the message history for a thread."""
    try:
        messages = await langgraph_client.get_thread_messages(thread_id)
        return {"messages": messages}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/thread/{thread_id}")
async def delete_thread(thread_id: str):
    """Delete a conversation thread."""
    try:
        await langgraph_client.delete_thread(thread_id)
        return {"status": "deleted", "thread_id": thread_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
