LLM_BASE_URL=http://localhost:11434/v1

# Required: Model name to use
# Examples: llama3.2, mistral, codellama, etc.
LLM_MODEL=qwen3-coder-30b-a3b-instruct-mlx

# Optional: API key (some endpoints like Ollama don't require one)
# Use "ollama" for Ollama, "lm-studio" for LM Studio, or your actual key
LLM_API_KEY=ollama

# Wallet URL for ecash token redemption
# Points to the FastAPI backend wallet service
# The agent will redeem received ecash tokens to this wallet after successful processing
WALLET_URL=http://localhost:8000/api/wallet

# LangSmith tracing (optional)
LANGSMITH_API_KEY=lsv2_...
LANGCHAIN_PROJECT=PROJECTNAME...
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_TRACING_V2=true

DEBUG=1

TAVILY_API_KEY=not_set
XAI_API_KEY=this_will_be_used
OPENAI_API_KEY=not_used
ANTHROPIC_API_KEY=not_used
GOOGLE_API_KEY=not_used

# Should be set to true for a production deployment on Open Agent Platform. Should be set to false otherwise, such as for local development.
GET_API_KEYS_FROM_CONFIG=false